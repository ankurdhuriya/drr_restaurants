{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uwu2wY-gT90a"
   },
   "outputs": [],
   "source": [
    "from model import Actor, Critic, DRRAveStateRepresentation, PMF\n",
    "from learn import DRRTrainer\n",
    "from utils.general import csv_plot\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJgM7skZmj2H",
    "outputId": "d98cdd53-85c0-4120-ff40-52dd6d994737"
   },
   "outputs": [],
   "source": [
    "class config():\n",
    "    output_path = 'results/' + datetime.datetime.now().strftime('%y%m%d-%H%M%S') + '/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    plot_dir = output_path + 'rewards.pdf'\n",
    "\n",
    "    train_actor_loss_data_dir = output_path + 'train_actor_loss_data.npy'\n",
    "    train_critic_loss_data_dir = output_path + 'train_critic_loss_data.npy'\n",
    "    train_mean_reward_data_dir = output_path + 'train_mean_reward_data.npy'\n",
    "\n",
    "    train_actor_loss_plot_dir = output_path + 'train_actor_loss.png'\n",
    "    train_critic_loss_plot_dir = output_path + 'train_critic_loss.png'\n",
    "    train_mean_reward_plot_dir = output_path + 'train_mean_reward.png'\n",
    "\n",
    "    trained_models_dir = 'trained/'\n",
    "\n",
    "    actor_model_trained = trained_models_dir + 'actor_net.weights'\n",
    "    critic_model_trained = trained_models_dir + 'critic_net.weights'\n",
    "    state_rep_model_trained = trained_models_dir + 'state_rep_net.weights'\n",
    "\n",
    "    actor_model_dir = output_path + 'actor_net.weights'\n",
    "    critic_model_dir = output_path + 'critic_net.weights'\n",
    "    state_rep_model_dir = output_path + 'state_rep_net.weights'\n",
    "\n",
    "    csv_dir = output_path + 'log.csv'\n",
    "\n",
    "    path_to_trained_pmf = trained_models_dir + 'yelp_ratio_0.800000_bs_1024_e_93_wd_0.100000_lr_0.000100_trained_pmf.pt'\n",
    "\n",
    "    # hyperparams\n",
    "    batch_size = 64\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 0 #5000\n",
    "    learning_freq = 1\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    max_timesteps_train = 12490 #260000\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 20000\n",
    "    embedding_feature_size = 100\n",
    "    episode_length = 10\n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 100\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    "\n",
    "    no_cuda = True\n",
    "\n",
    "def seed_all(cuda, seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJgM7skZmj2H",
    "outputId": "d98cdd53-85c0-4120-ff40-52dd6d994737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DRR Framework ----------------------------------------------------------------------------\n",
      "Using CPU\n",
      "Seeds initialized\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print(\"Initializing DRR Framework ----------------------------------------------------------------------------\")\n",
    " \n",
    "# Get CUDA device if available\n",
    "cuda = True if not config.no_cuda and torch.cuda.is_available() else False\n",
    "print(\"Using CUDA\") if cuda else print(\"Using CPU\")\n",
    " \n",
    " \n",
    "# Init seeds\n",
    "seed_all(cuda, 0)\n",
    "print(\"Seeds initialized\")\n",
    " \n",
    "# Grab models\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "state_rep_function = DRRAveStateRepresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJgM7skZmj2H",
    "outputId": "d98cdd53-85c0-4120-ff40-52dd6d994737"
   },
   "outputs": [],
   "source": [
    " \n",
    "# Import Data\n",
    "users = pickle.load(open('dataset/user_id_to_num.pkl', 'rb'))\n",
    "items = pickle.load(open('dataset/rest_id_to_num.pkl', 'rb'))\n",
    "data = np.load('dataset/data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJgM7skZmj2H",
    "outputId": "d98cdd53-85c0-4120-ff40-52dd6d994737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported, shuffled, and split into Train/Test, ratio= 0.8\n",
      "Train data shape:  torch.Size([83880, 4])\n",
      "Test data shape:  torch.Size([20970, 4])\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "train_data = torch.from_numpy(data[:int(config.train_ratio * data.shape[0])])\n",
    "test_data = torch.from_numpy(data[int(config.train_ratio * data.shape[0]):])\n",
    "print(\"Data imported, shuffled, and split into Train/Test, ratio=\", config.train_ratio)\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Test data shape: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_data[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJgM7skZmj2H",
    "outputId": "d98cdd53-85c0-4120-ff40-52dd6d994737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized PMF, imported weights, created reward_function\n",
      "Extracted user and item embeddings from PMF\n",
      "User embeddings shape:  torch.Size([1245, 100])\n",
      "Item embeddings shape:  torch.Size([40829, 100])\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Create and load PMF function for rewards and embeddings\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "reward_function = PMF(n_users, n_items, config.embedding_feature_size, is_sparse=False, no_cuda=~cuda)\n",
    "reward_function.load_state_dict(torch.load(config.path_to_trained_pmf))\n",
    " \n",
    "# Freeze all the parameters in the network\n",
    "for param in reward_function.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Initialized PMF, imported weights, created reward_function\")\n",
    " \n",
    "# Extract embeddings\n",
    "user_embeddings = reward_function.user_embeddings.weight.data\n",
    "item_embeddings = reward_function.item_embeddings.weight.data\n",
    "print(\"Extracted user and item embeddings from PMF\")\n",
    "print(\"User embeddings shape: \", user_embeddings.shape)\n",
    "print(\"Item embeddings shape: \", item_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJgM7skZmj2H",
    "outputId": "d98cdd53-85c0-4120-ff40-52dd6d994737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DRRTrainer -------------------------------------------------------------------------------\n",
      "Current PyTorch Device:  cpu\n",
      "Data dimensions extracted\n",
      "Models initialized\n",
      "Model weights initialized, copied to target\n",
      "Optimizers initialized\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Init trainer\n",
    "print(\"Initializing DRRTrainer -------------------------------------------------------------------------------\")\n",
    "trainer = DRRTrainer(config,\n",
    "                      actor_function,\n",
    "                      critic_function,\n",
    "                      state_rep_function,\n",
    "                      reward_function,\n",
    "                      users,\n",
    "                      items,\n",
    "                      train_data,\n",
    "                      test_data,\n",
    "                      user_embeddings,\n",
    "                      item_embeddings,\n",
    "                      cuda\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJgM7skZmj2H",
    "outputId": "d98cdd53-85c0-4120-ff40-52dd6d994737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DRRTrainer.learn() ---------------------------------------------------------------------------\n",
      "User id 1151, Episode 6, Timestamp 6, rec item 30189, reward 4.5425920486450195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankurdhuriya/RL_UseCase/drr_restaurants/learn.py:249: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ignored_items.append(torch.tensor(rec_item_idx).to(self.device))\n",
      "/Users/ankurdhuriya/miniconda3/envs/drr_pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 100 | Episode 9 | Mean Ep R 5.0282 | Max R 5.0282 | Critic Params Norm 6.7637 | Actor Loss -5.3609 | Critic Loss 0.2921 | \n",
      "Timestep 200 | Episode 19 | Mean Ep R 4.5080 | Max R 4.5080 | Critic Params Norm 10.4625 | Actor Loss -2.2841 | Critic Loss 1.9314 | \n",
      "Timestep 300 | Episode 29 | Mean Ep R 4.3601 | Max R 4.3601 | Critic Params Norm 1.6217 | Actor Loss -13.7468 | Critic Loss 0.0203 | \n",
      "Timestep 400 | Episode 39 | Mean Ep R 4.2466 | Max R 4.2466 | Critic Params Norm 6.8176 | Actor Loss -16.2873 | Critic Loss 0.0668 | \n",
      "Timestep 500 | Episode 49 | Mean Ep R 4.0670 | Max R 4.0670 | Critic Params Norm 7.7448 | Actor Loss -20.8552 | Critic Loss 0.0618 | \n",
      "Timestep 600 | Episode 59 | Mean Ep R 3.9421 | Max R 3.9421 | Critic Params Norm 37.3074 | Actor Loss -23.8011 | Critic Loss 0.9573 | \n",
      "Timestep 700 | Episode 69 | Mean Ep R 4.9089 | Max R 4.9089 | Critic Params Norm 31.3349 | Actor Loss -22.7429 | Critic Loss 0.9133 | \n",
      "Timestep 800 | Episode 79 | Mean Ep R 3.6262 | Max R 3.6262 | Critic Params Norm 54.6728 | Actor Loss -27.1349 | Critic Loss 2.0636 | \n",
      "Timestep 900 | Episode 89 | Mean Ep R 4.5453 | Max R 4.5453 | Critic Params Norm 33.1412 | Actor Loss -26.9876 | Critic Loss 0.6407 | \n",
      "Timestep 1000 | Episode 99 | Mean Ep R 4.9036 | Max R 4.9036 | Critic Params Norm 5.0873 | Actor Loss -31.2929 | Critic Loss 0.0485 | \n",
      "Timestep 1100 | Episode 109 | Mean Ep R 3.8897 | Max R 3.8897 | Critic Params Norm 25.9565 | Actor Loss -32.7837 | Critic Loss 0.3080 | \n",
      "Timestep 1200 | Episode 119 | Mean Ep R 3.6595 | Max R 3.6595 | Critic Params Norm 2.9394 | Actor Loss -34.3479 | Critic Loss 0.0343 | \n",
      "Timestep 1300 | Episode 129 | Mean Ep R 4.6214 | Max R 4.6214 | Critic Params Norm 30.3348 | Actor Loss -35.3213 | Critic Loss 0.5433 | \n",
      "Timestep 1400 | Episode 139 | Mean Ep R 4.6513 | Max R 4.6513 | Critic Params Norm 11.8927 | Actor Loss -34.7512 | Critic Loss 0.0942 | \n",
      "Timestep 1500 | Episode 149 | Mean Ep R 5.0000 | Max R 5.0000 | Critic Params Norm 93.1835 | Actor Loss -42.8778 | Critic Loss 3.1339 | \n",
      "Timestep 1600 | Episode 159 | Mean Ep R 4.9679 | Max R 4.9679 | Critic Params Norm 26.5693 | Actor Loss -40.6639 | Critic Loss 0.2949 | \n",
      "Timestep 1700 | Episode 169 | Mean Ep R 5.4691 | Max R 5.4691 | Critic Params Norm 25.4073 | Actor Loss -40.9281 | Critic Loss 0.2774 | \n",
      "Timestep 1800 | Episode 179 | Mean Ep R 4.7245 | Max R 4.7245 | Critic Params Norm 6.6023 | Actor Loss -43.0530 | Critic Loss 0.1225 | \n",
      "Timestep 1900 | Episode 189 | Mean Ep R 4.5264 | Max R 4.5264 | Critic Params Norm 36.0770 | Actor Loss -44.2322 | Critic Loss 0.5333 | \n",
      "Timestep 2000 | Episode 199 | Mean Ep R 3.8901 | Max R 3.8901 | Critic Params Norm 7.9393 | Actor Loss -44.4520 | Critic Loss 0.0871 | \n",
      "Timestep 2100 | Episode 209 | Mean Ep R 4.5396 | Max R 4.5396 | Critic Params Norm 39.8872 | Actor Loss -44.8954 | Critic Loss 0.7455 | \n",
      "Timestep 2200 | Episode 219 | Mean Ep R 4.4618 | Max R 4.4618 | Critic Params Norm 24.8187 | Actor Loss -47.8284 | Critic Loss 0.3051 | \n",
      "Timestep 2300 | Episode 229 | Mean Ep R 3.6946 | Max R 3.6946 | Critic Params Norm 27.3623 | Actor Loss -45.9689 | Critic Loss 0.4470 | \n",
      "Timestep 2400 | Episode 239 | Mean Ep R 3.6585 | Max R 3.6585 | Critic Params Norm 19.1798 | Actor Loss -46.7233 | Critic Loss 0.1935 | \n",
      "Timestep 2500 | Episode 249 | Mean Ep R 4.9936 | Max R 4.9936 | Critic Params Norm 23.8180 | Actor Loss -46.1617 | Critic Loss 0.2917 | \n",
      "Timestep 2600 | Episode 259 | Mean Ep R 3.9326 | Max R 3.9326 | Critic Params Norm 42.8086 | Actor Loss -48.3242 | Critic Loss 0.5363 | \n",
      "Timestep 2700 | Episode 269 | Mean Ep R 3.2525 | Max R 3.2525 | Critic Params Norm 7.8465 | Actor Loss -51.2250 | Critic Loss 0.2302 | \n",
      "Timestep 2800 | Episode 279 | Mean Ep R 4.9013 | Max R 4.9013 | Critic Params Norm 14.3450 | Actor Loss -49.7611 | Critic Loss 0.2309 | \n",
      "Timestep 2900 | Episode 289 | Mean Ep R 4.1524 | Max R 4.1524 | Critic Params Norm 20.1847 | Actor Loss -51.3823 | Critic Loss 0.1880 | \n",
      "Timestep 3000 | Episode 299 | Mean Ep R 3.7173 | Max R 3.7173 | Critic Params Norm 18.9611 | Actor Loss -50.2145 | Critic Loss 0.2600 | \n",
      "Timestep 3100 | Episode 309 | Mean Ep R 4.8116 | Max R 4.8116 | Critic Params Norm 5.4779 | Actor Loss -51.6074 | Critic Loss 0.0939 | \n",
      "Timestep 3200 | Episode 319 | Mean Ep R 3.4016 | Max R 3.4016 | Critic Params Norm 31.5672 | Actor Loss -52.5809 | Critic Loss 0.6222 | \n",
      "Timestep 3300 | Episode 329 | Mean Ep R 4.2141 | Max R 4.2141 | Critic Params Norm 68.7208 | Actor Loss -50.4843 | Critic Loss 1.6794 | \n",
      "Timestep 3400 | Episode 339 | Mean Ep R 4.4504 | Max R 4.4504 | Critic Params Norm 26.2128 | Actor Loss -53.1536 | Critic Loss 0.2886 | \n",
      "Timestep 3500 | Episode 349 | Mean Ep R 4.6809 | Max R 4.6809 | Critic Params Norm 3.6342 | Actor Loss -51.4511 | Critic Loss 0.1826 | \n",
      "Timestep 3600 | Episode 359 | Mean Ep R 3.3971 | Max R 3.3971 | Critic Params Norm 19.3978 | Actor Loss -53.0699 | Critic Loss 0.2476 | \n",
      "Timestep 3700 | Episode 369 | Mean Ep R 3.6536 | Max R 3.6536 | Critic Params Norm 18.6115 | Actor Loss -52.9460 | Critic Loss 0.2039 | \n",
      "Timestep 3800 | Episode 379 | Mean Ep R 4.7709 | Max R 4.7709 | Critic Params Norm 51.0448 | Actor Loss -55.1883 | Critic Loss 0.6524 | \n",
      "Timestep 3900 | Episode 389 | Mean Ep R 3.6179 | Max R 3.6179 | Critic Params Norm 32.2822 | Actor Loss -54.0235 | Critic Loss 0.4654 | \n",
      "Timestep 4000 | Episode 399 | Mean Ep R 3.3226 | Max R 3.3226 | Critic Params Norm 35.3513 | Actor Loss -55.1011 | Critic Loss 0.6330 | \n",
      "Timestep 4100 | Episode 409 | Mean Ep R 3.6051 | Max R 3.6051 | Critic Params Norm 34.6302 | Actor Loss -52.2379 | Critic Loss 0.4406 | \n",
      "Timestep 4200 | Episode 419 | Mean Ep R 2.1678 | Max R 2.1678 | Critic Params Norm 48.9874 | Actor Loss -57.1656 | Critic Loss 0.9330 | \n",
      "Timestep 4300 | Episode 429 | Mean Ep R 3.1621 | Max R 3.1621 | Critic Params Norm 56.2334 | Actor Loss -57.2826 | Critic Loss 0.7516 | \n",
      "Timestep 4400 | Episode 439 | Mean Ep R 5.8182 | Max R 5.8182 | Critic Params Norm 67.4202 | Actor Loss -53.9689 | Critic Loss 1.6562 | \n",
      "Timestep 4500 | Episode 449 | Mean Ep R 4.2888 | Max R 4.2888 | Critic Params Norm 98.8867 | Actor Loss -56.3941 | Critic Loss 2.0111 | \n",
      "Timestep 4600 | Episode 459 | Mean Ep R 3.8497 | Max R 3.8497 | Critic Params Norm 31.9019 | Actor Loss -56.2593 | Critic Loss 0.4652 | \n",
      "Timestep 4700 | Episode 469 | Mean Ep R 3.8554 | Max R 3.8554 | Critic Params Norm 37.3043 | Actor Loss -55.2145 | Critic Loss 0.5051 | \n",
      "Timestep 4800 | Episode 479 | Mean Ep R 4.6726 | Max R 4.6726 | Critic Params Norm 23.2295 | Actor Loss -55.1388 | Critic Loss 0.3889 | \n",
      "Timestep 4900 | Episode 489 | Mean Ep R 4.5836 | Max R 4.5836 | Critic Params Norm 23.1911 | Actor Loss -56.0233 | Critic Loss 0.3786 | \n",
      "Timestep 5000 | Episode 499 | Mean Ep R 3.7967 | Max R 3.7967 | Critic Params Norm 19.2346 | Actor Loss -57.4629 | Critic Loss 0.2827 | \n",
      "Timestep 5100 | Episode 509 | Mean Ep R 2.9265 | Max R 2.9265 | Critic Params Norm 30.2088 | Actor Loss -56.2232 | Critic Loss 0.3627 | \n",
      "Timestep 5200 | Episode 519 | Mean Ep R 4.5427 | Max R 4.5427 | Critic Params Norm 64.9315 | Actor Loss -55.4403 | Critic Loss 1.4501 | \n",
      "Timestep 5300 | Episode 529 | Mean Ep R 5.1106 | Max R 5.1106 | Critic Params Norm 9.8444 | Actor Loss -54.0015 | Critic Loss 0.1122 | \n",
      "Timestep 5400 | Episode 539 | Mean Ep R 4.3511 | Max R 4.3511 | Critic Params Norm 6.7970 | Actor Loss -55.1935 | Critic Loss 0.3995 | \n",
      "Timestep 5500 | Episode 549 | Mean Ep R 4.0620 | Max R 4.0620 | Critic Params Norm 56.4374 | Actor Loss -58.1426 | Critic Loss 0.9077 | \n",
      "Timestep 5600 | Episode 559 | Mean Ep R 3.3783 | Max R 3.3783 | Critic Params Norm 3.7749 | Actor Loss -54.8215 | Critic Loss 0.1806 | \n",
      "Timestep 5700 | Episode 569 | Mean Ep R 4.3638 | Max R 4.3638 | Critic Params Norm 32.0225 | Actor Loss -55.8579 | Critic Loss 0.3976 | \n",
      "Timestep 5800 | Episode 579 | Mean Ep R 3.6278 | Max R 3.6278 | Critic Params Norm 77.6410 | Actor Loss -52.3498 | Critic Loss 1.7200 | \n",
      "Timestep 5900 | Episode 589 | Mean Ep R 4.8707 | Max R 4.8707 | Critic Params Norm 103.1793 | Actor Loss -55.9643 | Critic Loss 1.7847 | \n",
      "Timestep 6000 | Episode 599 | Mean Ep R 4.2569 | Max R 4.2569 | Critic Params Norm 42.8003 | Actor Loss -53.3982 | Critic Loss 0.5726 | \n",
      "Timestep 6100 | Episode 609 | Mean Ep R 4.5979 | Max R 4.5979 | Critic Params Norm 10.1829 | Actor Loss -52.1986 | Critic Loss 0.2131 | \n",
      "Timestep 6200 | Episode 619 | Mean Ep R 4.0000 | Max R 4.0000 | Critic Params Norm 20.6630 | Actor Loss -52.3735 | Critic Loss 0.3014 | \n",
      "Timestep 6300 | Episode 629 | Mean Ep R 3.1889 | Max R 3.1889 | Critic Params Norm 111.0223 | Actor Loss -55.3596 | Critic Loss 2.3062 | \n",
      "Timestep 6400 | Episode 639 | Mean Ep R 5.0813 | Max R 5.0813 | Critic Params Norm 3.1377 | Actor Loss -54.3864 | Critic Loss 0.1034 | \n",
      "Timestep 6500 | Episode 649 | Mean Ep R 4.5520 | Max R 4.5520 | Critic Params Norm 3.3951 | Actor Loss -52.4119 | Critic Loss 0.1375 | \n",
      "Timestep 6600 | Episode 659 | Mean Ep R 4.1405 | Max R 4.1405 | Critic Params Norm 10.8405 | Actor Loss -54.1470 | Critic Loss 0.1270 | \n",
      "Timestep 6700 | Episode 669 | Mean Ep R 3.7254 | Max R 3.7254 | Critic Params Norm 58.0350 | Actor Loss -54.2569 | Critic Loss 0.8846 | \n",
      "Timestep 6800 | Episode 679 | Mean Ep R 1.6371 | Max R 1.6371 | Critic Params Norm 49.6824 | Actor Loss -51.8957 | Critic Loss 0.9621 | \n",
      "Timestep 6900 | Episode 689 | Mean Ep R 4.6507 | Max R 4.6507 | Critic Params Norm 10.6739 | Actor Loss -54.0772 | Critic Loss 0.1329 | \n",
      "Timestep 7000 | Episode 699 | Mean Ep R 4.5367 | Max R 4.5367 | Critic Params Norm 19.7901 | Actor Loss -53.9969 | Critic Loss 0.2938 | \n",
      "Timestep 7100 | Episode 709 | Mean Ep R 5.0606 | Max R 5.0606 | Critic Params Norm 4.1506 | Actor Loss -53.5627 | Critic Loss 0.1407 | \n",
      "Timestep 7200 | Episode 719 | Mean Ep R 3.9097 | Max R 3.9097 | Critic Params Norm 84.4128 | Actor Loss -52.1887 | Critic Loss 1.7516 | \n",
      "Timestep 7300 | Episode 729 | Mean Ep R 3.3168 | Max R 3.3168 | Critic Params Norm 5.5362 | Actor Loss -52.6368 | Critic Loss 0.1541 | \n",
      "Timestep 7400 | Episode 739 | Mean Ep R 3.9495 | Max R 3.9495 | Critic Params Norm 23.1459 | Actor Loss -52.8813 | Critic Loss 0.2282 | \n",
      "Timestep 7500 | Episode 749 | Mean Ep R 3.9473 | Max R 3.9473 | Critic Params Norm 39.8334 | Actor Loss -55.0510 | Critic Loss 0.5548 | \n",
      "Timestep 7600 | Episode 759 | Mean Ep R 4.3184 | Max R 4.3184 | Critic Params Norm 83.5210 | Actor Loss -52.6413 | Critic Loss 1.6566 | \n",
      "Timestep 7700 | Episode 769 | Mean Ep R 4.5869 | Max R 4.5869 | Critic Params Norm 10.5991 | Actor Loss -51.2887 | Critic Loss 0.3074 | \n",
      "Timestep 7800 | Episode 779 | Mean Ep R 3.9636 | Max R 3.9636 | Critic Params Norm 33.7138 | Actor Loss -55.0270 | Critic Loss 0.2895 | \n",
      "Timestep 7900 | Episode 789 | Mean Ep R 4.0776 | Max R 4.0776 | Critic Params Norm 54.5623 | Actor Loss -52.3801 | Critic Loss 0.6766 | \n",
      "Timestep 8000 | Episode 799 | Mean Ep R 3.5102 | Max R 3.5102 | Critic Params Norm 14.6979 | Actor Loss -51.4996 | Critic Loss 0.1288 | \n",
      "Timestep 8100 | Episode 809 | Mean Ep R 2.9264 | Max R 2.9264 | Critic Params Norm 64.4732 | Actor Loss -53.7752 | Critic Loss 1.1739 | \n",
      "Timestep 8200 | Episode 819 | Mean Ep R 3.8998 | Max R 3.8998 | Critic Params Norm 48.5407 | Actor Loss -52.7541 | Critic Loss 0.6713 | \n",
      "Timestep 8300 | Episode 829 | Mean Ep R 5.0353 | Max R 5.0353 | Critic Params Norm 49.8408 | Actor Loss -52.3789 | Critic Loss 0.8351 | \n",
      "Timestep 8400 | Episode 839 | Mean Ep R 4.0136 | Max R 4.0136 | Critic Params Norm 7.3599 | Actor Loss -54.9569 | Critic Loss 0.1855 | \n",
      "Timestep 8500 | Episode 849 | Mean Ep R 6.3889 | Max R 6.3889 | Critic Params Norm 29.2175 | Actor Loss -52.5870 | Critic Loss 0.3793 | \n",
      "Timestep 8600 | Episode 859 | Mean Ep R 4.4204 | Max R 4.4204 | Critic Params Norm 9.9612 | Actor Loss -53.3498 | Critic Loss 0.3240 | \n",
      "Timestep 8700 | Episode 869 | Mean Ep R 4.0000 | Max R 4.0000 | Critic Params Norm 33.5561 | Actor Loss -54.0067 | Critic Loss 0.5118 | \n",
      "Timestep 8800 | Episode 879 | Mean Ep R 3.0000 | Max R 3.0000 | Critic Params Norm 11.0364 | Actor Loss -51.0422 | Critic Loss 0.1736 | \n",
      "Timestep 8900 | Episode 889 | Mean Ep R 3.5049 | Max R 3.5049 | Critic Params Norm 66.8127 | Actor Loss -50.5995 | Critic Loss 1.3810 | \n",
      "Timestep 9000 | Episode 899 | Mean Ep R 4.2036 | Max R 4.2036 | Critic Params Norm 52.8324 | Actor Loss -50.7589 | Critic Loss 0.6614 | \n",
      "Timestep 9100 | Episode 909 | Mean Ep R 3.5568 | Max R 3.5568 | Critic Params Norm 32.0398 | Actor Loss -53.0161 | Critic Loss 0.4437 | \n",
      "Timestep 9200 | Episode 919 | Mean Ep R 4.2852 | Max R 4.2852 | Critic Params Norm 64.7472 | Actor Loss -50.7489 | Critic Loss 1.0757 | \n",
      "Timestep 9300 | Episode 929 | Mean Ep R 4.7326 | Max R 4.7326 | Critic Params Norm 5.7789 | Actor Loss -51.2162 | Critic Loss 0.2251 | \n",
      "Timestep 9400 | Episode 939 | Mean Ep R 2.4385 | Max R 2.4385 | Critic Params Norm 99.3014 | Actor Loss -53.2173 | Critic Loss 1.9107 | \n",
      "Timestep 9500 | Episode 949 | Mean Ep R 4.3853 | Max R 4.3853 | Critic Params Norm 26.9881 | Actor Loss -51.9531 | Critic Loss 0.3077 | \n",
      "Timestep 9600 | Episode 959 | Mean Ep R 4.3970 | Max R 4.3970 | Critic Params Norm 10.2725 | Actor Loss -51.8228 | Critic Loss 0.3491 | \n",
      "Timestep 9700 | Episode 969 | Mean Ep R 4.4551 | Max R 4.4551 | Critic Params Norm 9.9494 | Actor Loss -52.0729 | Critic Loss 0.2051 | \n",
      "Timestep 9800 | Episode 979 | Mean Ep R 3.7342 | Max R 3.7342 | Critic Params Norm 58.9530 | Actor Loss -50.9674 | Critic Loss 0.9329 | \n",
      "Timestep 9900 | Episode 989 | Mean Ep R 3.5263 | Max R 3.5263 | Critic Params Norm 22.3471 | Actor Loss -50.4331 | Critic Loss 0.3306 | \n",
      "Timestep 10000 | Episode 999 | Mean Ep R 4.4621 | Max R 4.4621 | Critic Params Norm 55.4878 | Actor Loss -49.9062 | Critic Loss 0.7875 | \n",
      "Timestep 10100 | Episode 1009 | Mean Ep R 4.4992 | Max R 4.4992 | Critic Params Norm 6.5097 | Actor Loss -52.3953 | Critic Loss 0.2246 | \n",
      "Training Finishedode 9, Timestamp 9, rec item 33578, reward 4.421388626098633\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"Starting DRRTrainer.learn() ---------------------------------------------------------------------------\")\n",
    "actor_losses, critic_losses, epi_avg_rewards = trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e7WsofyYJs4A"
   },
   "outputs": [],
   "source": [
    "# Change to newest trained data directories\n",
    "# config.trained_models_dir = config.output_path\n",
    "# output_path = config.output_path\n",
    "config.trained_models_dir = \"results/220620-122520/\"\n",
    "output_path = \"results/220620-122520/\"\n",
    "\n",
    "train_actor_loss_data_dir = output_path + 'train_actor_loss_data.npy'\n",
    "train_critic_loss_data_dir = output_path + 'train_critic_loss_data.npy'\n",
    "train_mean_reward_data_dir = output_path + 'train_mean_reward_data.npy'\n",
    "\n",
    "config.actor_model_trained = config.trained_models_dir + 'actor_net.weights'\n",
    "config.critic_model_trained = config.trained_models_dir + 'critic_net.weights'\n",
    "config.state_rep_model_trained = config.trained_models_dir + 'state_rep_net.weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTh6gtCGQKdM"
   },
   "source": [
    "Create and save smoothened graphs of losses and average rewards for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVrzFMYyPot7",
    "outputId": "cc62d7f4-c9fa-4448-9b62-227bd062672b"
   },
   "outputs": [],
   "source": [
    "def noiseless_plot(y, title, ylabel, save_loc):\n",
    "    # operate smoothing\n",
    "    smoother = ConvolutionSmoother(window_len=1000, window_type='ones')\n",
    "    smoother.smooth(y)\n",
    "\n",
    "    # generate intervals\n",
    "    low, up = smoother.get_intervals('sigma_interval', n_sigma=3)\n",
    "\n",
    "    # plot the smoothed timeseries with intervals\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(11,6))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.plot(smoother.data[0], color='orange')\n",
    "    plt.plot(smoother.smooth_data[0], linewidth=3, color='blue')\n",
    "    plt.fill_between(range(len(smoother.data[0])), low[0], up[0], alpha=0.3)\n",
    "    plt.savefig(save_loc)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Js_AsydTL4rX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tsmoothie.smoother import ConvolutionSmoother\n",
    "\n",
    "actor_losses = np.load(train_actor_loss_data_dir)\n",
    "critic_losses = np.load(train_critic_loss_data_dir)\n",
    "epi_avg_rewards = np.load(train_mean_reward_data_dir)\n",
    "\n",
    "noiseless_plot(actor_losses, \n",
    "               \"Actor Loss (Train)\", \n",
    "               \"Actor Loss (Train)\", \n",
    "               output_path + \"train_actor_loss_smooth.png\")\n",
    "               \n",
    "noiseless_plot(critic_losses, \n",
    "               \"Critic Loss (Train)\", \n",
    "               \"Critic Loss (Train)\", \n",
    "               output_path + \"train_critic_loss_smooth.png\")\n",
    "\n",
    "noiseless_plot(epi_avg_rewards, \n",
    "               \"Mean Reward (Train)\", \n",
    "               \"Mean Reward (Train)\", \n",
    "               output_path + \"train_mean_reward_smooth.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpdq8n7aQFag"
   },
   "source": [
    "Save hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V0TuQQlwYDhM"
   },
   "outputs": [],
   "source": [
    "sourceFile = open(output_path + \"hyperparams.txt\", 'w')\n",
    "print(config.__dict__, file = sourceFile)\n",
    "sourceFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCzuHAUiWpTO"
   },
   "source": [
    "**Run** Offline and Online evaluations, save scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_drr.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
