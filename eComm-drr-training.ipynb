{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99fded02-3f5e-4889-9598-27d5e1fe68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Actor, Critic, DRRAveStateRepresentation, PMF\n",
    "from learn import DRRTrainer\n",
    "from utils.general import csv_plot\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c007b27d-5879-47a8-8a08-9ca70650dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    " class config():\n",
    "    output_path = 'results/' + datetime.datetime.now().strftime('%y%m%d-%H%M%S') + '/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    plot_dir = output_path + 'rewards.pdf'\n",
    " \n",
    "    train_actor_loss_data_dir = output_path + 'train_actor_loss_data.npy'\n",
    "    train_critic_loss_data_dir = output_path + 'train_critic_loss_data.npy'\n",
    "    train_mean_reward_data_dir = output_path + 'train_mean_reward_data.npy'\n",
    " \n",
    "    train_actor_loss_plot_dir = output_path + 'train_actor_loss.png'\n",
    "    train_critic_loss_plot_dir = output_path + 'train_critic_loss.png'\n",
    "    train_mean_reward_plot_dir = output_path + 'train_mean_reward.png'\n",
    " \n",
    "    trained_models_dir = 'trained/'\n",
    " \n",
    "    actor_model_trained = trained_models_dir + 'actor_net.weights'\n",
    "    critic_model_trained = trained_models_dir + 'critic_net.weights'\n",
    "    state_rep_model_trained = trained_models_dir + 'state_rep_net.weights'\n",
    " \n",
    "    actor_model_dir = output_path + 'actor_net.weights'\n",
    "    critic_model_dir = output_path + 'critic_net.weights'\n",
    "    state_rep_model_dir = output_path + 'state_rep_net.weights'\n",
    " \n",
    "    csv_dir = output_path + 'log.csv'\n",
    " \n",
    "    path_to_trained_pmf = trained_models_dir + 'eComm_ratio_0.800000_bs_256_e_25_wd_0.100000_lr_0.000100_trained_pmf.pt'\n",
    " \n",
    "    # hyperparams\n",
    "    batch_size = 128\n",
    "    gamma = 0.9\n",
    "    replay_buffer_size = 100000\n",
    "    history_buffer_size = 5\n",
    "    learning_start = 1000 #500\n",
    "    learning_freq = 1\n",
    "    lr_state_rep = 0.001\n",
    "    lr_actor = 0.0001\n",
    "    lr_critic = 0.001\n",
    "    eps_start = 1\n",
    "    eps = 0.1\n",
    "    eps_steps = 10000\n",
    "    eps_eval = 0.1\n",
    "    tau = 0.01 # inital 0.001\n",
    "    beta = 0.4\n",
    "    prob_alpha = 0.3\n",
    "    max_timesteps_train = 53090\n",
    "    max_epochs_offline = 500\n",
    "    max_timesteps_online = 1000\n",
    "    embedding_feature_size = 100\n",
    "    episode_length = 10\n",
    "    train_ratio = 0.8\n",
    "    weight_decay = 0.01\n",
    "    clip_val = 1.0\n",
    "    log_freq = 1000\n",
    "    saving_freq = 1000\n",
    "    zero_reward = False\n",
    " \n",
    "    no_cuda = True\n",
    "    \n",
    "    logs_dir = 'runs/training2'\n",
    "\n",
    "def seed_all(cuda, seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.manual_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1919ca3-3a5c-4ef5-85c6-8646fa1de5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DRR Framework ----------------------------------------------------------------------------\n",
      "Using CPU\n",
      "Seeds initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing DRR Framework ----------------------------------------------------------------------------\")\n",
    " \n",
    "# Get CUDA device if available\n",
    "cuda = True if not config.no_cuda and torch.cuda.is_available() else False\n",
    "print(\"Using CUDA\") if cuda else print(\"Using CPU\")\n",
    " \n",
    " \n",
    "# Init seeds\n",
    "seed_all(cuda, 0)\n",
    "print(\"Seeds initialized\")\n",
    " \n",
    "# Grab models\n",
    "actor_function = Actor\n",
    "critic_function = Critic\n",
    "state_rep_function = DRRAveStateRepresentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2eb55ab-cebb-44b9-abf7-7b32302ae5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Import Data\n",
    "\n",
    "data_df = pd.read_csv('dataset/eComm-sample-data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a215558-3825-44f4-85da-38710555b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5309 15184\n"
     ]
    }
   ],
   "source": [
    "event_type_to_num = {'view': 1, 'cart': 2, 'purchase': 3}\n",
    "data_df['behavior'] = data_df['event_type'].apply(lambda x : event_type_to_num[x])\n",
    "\n",
    "items = dict(zip(data_df['product_id'], data_df['product_id_num']))\n",
    "users = dict(zip(data_df['user_id'], data_df['user_id_num']))\n",
    "\n",
    "NUM_USERS, NUM_ITEMS = len(users), len(items)\n",
    "print(NUM_USERS, NUM_ITEMS)\n",
    "\n",
    "data = data_df.loc[:, ['user_id_num', 'product_id_num', 'behavior', 'event_time']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62dc68f7-a89e-4c10-bd64-fec8cc16030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d035a9-dadf-45aa-bd28-8e41b326ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported, shuffled, and split into Train/Test, ratio= 0.8\n",
      "Train data shape:  torch.Size([100427, 4])\n",
      "Test data shape:  torch.Size([25107, 4])\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "train_data = torch.from_numpy(data[:int(config.train_ratio * data.shape[0])])\n",
    "test_data = torch.from_numpy(data[int(config.train_ratio * data.shape[0]):])\n",
    "print(\"Data imported, shuffled, and split into Train/Test, ratio=\", config.train_ratio)\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "024f6344-74fe-4149-a852-7951d0e7e21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized PMF, imported weights, created reward_function\n",
      "Extracted user and item embeddings from PMF\n",
      "User embeddings shape:  torch.Size([5309, 100])\n",
      "Item embeddings shape:  torch.Size([15184, 100])\n"
     ]
    }
   ],
   "source": [
    "# Create and load PMF function for rewards and embeddings\n",
    "reward_function = PMF(NUM_USERS, NUM_ITEMS, config.embedding_feature_size, is_sparse=False, no_cuda=~cuda)\n",
    "reward_function.load_state_dict(torch.load(config.path_to_trained_pmf))\n",
    " \n",
    "# Freeze all the parameters in the network\n",
    "for param in reward_function.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Initialized PMF, imported weights, created reward_function\")\n",
    " \n",
    "# Extract embeddings\n",
    "user_embeddings = reward_function.user_embeddings.weight.data\n",
    "item_embeddings = reward_function.item_embeddings.weight.data\n",
    "print(\"Extracted user and item embeddings from PMF\")\n",
    "print(\"User embeddings shape: \", user_embeddings.shape)\n",
    "print(\"Item embeddings shape: \", item_embeddings.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c96c2f-fc23-4c11-b4b5-5351966ce305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DRRTrainer -------------------------------------------------------------------------------\n",
      "Current PyTorch Device:  cpu\n",
      "Data dimensions extracted\n",
      "Models initialized\n",
      "Model weights initialized, copied to target\n",
      "Optimizers initialized\n"
     ]
    }
   ],
   "source": [
    "# Init trainer\n",
    "print(\"Initializing DRRTrainer -------------------------------------------------------------------------------\")\n",
    "trainer = DRRTrainer(config,\n",
    "                      actor_function,\n",
    "                      critic_function,\n",
    "                      state_rep_function,\n",
    "                      reward_function,\n",
    "                      users,\n",
    "                      items,\n",
    "                      train_data,\n",
    "                      test_data,\n",
    "                      user_embeddings,\n",
    "                      item_embeddings,\n",
    "                      cuda\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db2f2add-2692-4d9c-a869-d2f7a92cd82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.learning_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985f3735-3f01-4abe-a401-e6770c5ba9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DRRTrainer.learn() ---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankurdhuriya/RL_UseCase/drr_restaurants/learn.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ignored_items.append(torch.tensor(rec_item_idx).to(self.device))\n",
      "/Users/ankurdhuriya/miniconda3/envs/drr_pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 1000 | Episode 99 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 3.8388 | Actor Loss -17.1838 | Critic Loss 0.0847 | \n",
      "Timestep 2000 | Episode 199 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 5.6057 | Actor Loss -23.8594 | Critic Loss 0.1027 | \n",
      "Timestep 3000 | Episode 299 | Mean Ep R 3.0000 | Max R 3.0000 | Critic Params Norm 46.5860 | Actor Loss -27.7754 | Critic Loss 1.1238 | \n",
      "Timestep 4000 | Episode 399 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 17.5064 | Actor Loss -29.1093 | Critic Loss 0.2372 | \n",
      "Timestep 5000 | Episode 499 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 13.9061 | Actor Loss -26.7860 | Critic Loss 0.1545 | \n",
      "Timestep 6000 | Episode 599 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 1.0089 | Actor Loss -28.2015 | Critic Loss 0.0602 | \n",
      "Timestep 7000 | Episode 699 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 30.4927 | Actor Loss -25.6073 | Critic Loss 0.3476 | \n",
      "Timestep 8000 | Episode 799 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 20.0870 | Actor Loss -25.7792 | Critic Loss 0.2107 | \n",
      "Timestep 9000 | Episode 899 | Mean Ep R 3.0000 | Max R 3.0000 | Critic Params Norm 56.4695 | Actor Loss -27.9550 | Critic Loss 0.7980 | \n",
      "Timestep 10000 | Episode 999 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 6.7960 | Actor Loss -25.8447 | Critic Loss 0.0595 | \n",
      "Timestep 11000 | Episode 1099 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 64.2976 | Actor Loss -24.3415 | Critic Loss 1.0813 | \n",
      "Timestep 12000 | Episode 1199 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 6.4703 | Actor Loss -24.0249 | Critic Loss 0.0908 | \n",
      "Timestep 13000 | Episode 1299 | Mean Ep R 3.0000 | Max R 3.0000 | Critic Params Norm 20.1897 | Actor Loss -24.0683 | Critic Loss 0.1380 | \n",
      "Timestep 14000 | Episode 1399 | Mean Ep R 1.0000 | Max R 1.0000 | Critic Params Norm 39.8467 | Actor Loss -24.9806 | Critic Loss 0.4316 | \n",
      "Timestep 15000 | Episode 1499 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 64.2259 | Actor Loss -25.0686 | Critic Loss 0.8398 | \n",
      "Timestep 16000 | Episode 1599 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 13.7475 | Actor Loss -25.4997 | Critic Loss 0.1181 | \n",
      "Timestep 17000 | Episode 1699 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 36.1602 | Actor Loss -24.7865 | Critic Loss 0.3226 | \n",
      "Timestep 18000 | Episode 1799 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 11.1284 | Actor Loss -24.8591 | Critic Loss 0.0960 | \n",
      "Timestep 19000 | Episode 1899 | Mean Ep R 1.0000 | Max R 1.0000 | Critic Params Norm 29.3508 | Actor Loss -24.3970 | Critic Loss 0.2338 | \n",
      "Timestep 20000 | Episode 1999 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 31.8976 | Actor Loss -23.7253 | Critic Loss 0.3213 | \n",
      "Timestep 21000 | Episode 2099 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 24.2603 | Actor Loss -23.8462 | Critic Loss 0.1627 | \n",
      "Timestep 22000 | Episode 2199 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 14.8795 | Actor Loss -23.7597 | Critic Loss 0.1021 | \n",
      "Timestep 23000 | Episode 2299 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 17.8553 | Actor Loss -24.3709 | Critic Loss 0.1180 | \n",
      "Timestep 24000 | Episode 2399 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 9.1913 | Actor Loss -23.3089 | Critic Loss 0.0963 | \n",
      "Timestep 25000 | Episode 2499 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 12.5761 | Actor Loss -23.4717 | Critic Loss 0.0951 | \n",
      "Timestep 26000 | Episode 2599 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 5.8772 | Actor Loss -22.8757 | Critic Loss 0.0663 | \n",
      "Timestep 27000 | Episode 2699 | Mean Ep R 2.0000 | Max R 2.0000 | Critic Params Norm 11.8505 | Actor Loss -23.2935 | Critic Loss 0.1011 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Train\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting DRRTrainer.learn() ---------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m actor_losses, critic_losses, epi_avg_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RL_UseCase/drr_restaurants/learn.py:284\u001b[0m, in \u001b[0;36mDRRTrainer.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# print(f\"User id {e}, Episode {epoch}, step {t}, timestamp {timesteps} rec item {rec_item_idx}, reward {reward.item()}\", end='\\r')\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \n\u001b[1;32m    277\u001b[0m \n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# TRAIN\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timesteps \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_start) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    280\u001b[0m         (\u001b[38;5;28mlen\u001b[39m(replay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m    281\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    282\u001b[0m         (timesteps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     critic_loss, actor_loss, critic_params_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    287\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# LOGGING\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     actor_losses\u001b[38;5;241m.\u001b[39mappend(actor_loss)\n",
      "File \u001b[0;32m~/RL_UseCase/drr_restaurants/learn.py:380\u001b[0m, in \u001b[0;36mDRRTrainer.training_step\u001b[0;34m(self, t, replay_buffer, training)\u001b[0m\n\u001b[1;32m    374\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(batch\u001b[38;5;241m.\u001b[39mreward)\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# ---------------------------- Update Critic Network ---------------------------- #\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Calculate Critic loss\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m critic_loss, new_priorities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_prioritized_dqn_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mreward_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Minimize loss, update parameters, update priorities\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/RL_UseCase/drr_restaurants/learn.py:467\u001b[0m, in \u001b[0;36mDRRTrainer.compute_prioritized_dqn_loss\u001b[0;34m(self, state_batch, action_batch, reward_batch, next_state_batch, weights)\u001b[0m\n\u001b[1;32m    464\u001b[0m y \u001b[38;5;241m=\u001b[39m reward_batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m q_target\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# Get Q values for current state\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m q_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m    470\u001b[0m loss \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m q_vals\n",
      "File \u001b[0;32m~/miniconda3/envs/drr_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/RL_UseCase/drr_restaurants/model.py:63\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[0;32m---> 63\u001b[0m     output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((action, output), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m     output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(output))\n",
      "File \u001b[0;32m~/miniconda3/envs/drr_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/drr_pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train\n",
    "print(\"Starting DRRTrainer.learn() ---------------------------------------------------------------------------\")\n",
    "actor_losses, critic_losses, epi_avg_rewards = trainer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5498e2b-bdd8-4ac6-991c-2d750bba6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.trained_models_dir = \"results/220621-192625/\"\n",
    "output_path = \"results/220621-192625/\"\n",
    "\n",
    "train_actor_loss_data_dir = output_path + 'train_actor_loss_data.npy'\n",
    "train_critic_loss_data_dir = output_path + 'train_critic_loss_data.npy'\n",
    "train_mean_reward_data_dir = output_path + 'train_mean_reward_data.npy'\n",
    "\n",
    "config.actor_model_trained = config.trained_models_dir + 'actor_net.weights'\n",
    "config.critic_model_trained = config.trained_models_dir + 'critic_net.weights'\n",
    "config.state_rep_model_trained = config.trained_models_dir + 'state_rep_net.weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db9e14f-5397-4c77-8786-49b0d9266455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tsmoothie.smoother import ConvolutionSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e49938f-8f8b-4780-a2f8-1dc2468779bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseless_plot(y, title, ylabel, save_loc):\n",
    "    smoother = ConvolutionSmoother(window_len=1000, window_type='ones')\n",
    "    smoother.smooth(y)\n",
    "\n",
    "    # generate intervals\n",
    "    low, up = smoother.get_intervals('sigma_interval', n_sigma=3)\n",
    "\n",
    "    # plot the smoothed timeseries with intervals\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(11,6))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.plot(smoother.data[0], color='orange')\n",
    "    plt.plot(smoother.smooth_data[0], linewidth=3, color='blue')\n",
    "    plt.fill_between(range(len(smoother.data[0])), low[0], up[0], alpha=0.3)\n",
    "    plt.savefig(save_loc)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de404e9f-daf7-4466-837d-e9f8b4fe0926",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_losses = np.load(train_actor_loss_data_dir)\n",
    "critic_losses = np.load(train_critic_loss_data_dir)\n",
    "epi_avg_rewards = np.load(train_mean_reward_data_dir)\n",
    "\n",
    "noiseless_plot(actor_losses, \n",
    "               \"Actor Loss (Train)\", \n",
    "               \"Actor Loss (Train)\", \n",
    "               output_path + \"train_actor_loss_smooth.png\")\n",
    "               \n",
    "noiseless_plot(critic_losses, \n",
    "               \"Critic Loss (Train)\", \n",
    "               \"Critic Loss (Train)\", \n",
    "               output_path + \"train_critic_loss_smooth.png\")\n",
    "\n",
    "noiseless_plot(epi_avg_rewards, \n",
    "               \"Mean Reward (Train)\", \n",
    "               \"Mean Reward (Train)\", \n",
    "               output_path + \"train_mean_reward_smooth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d8cba-e91c-4398-824d-18e7d33b5172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
